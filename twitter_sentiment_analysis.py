# -*- coding: utf-8 -*-
"""Twitter_Sentiment_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TdX4GYRHktYqBmKXMSTHrpz30FM-RAIf

#  IMPORTING LIBRARIES AND DATASETS
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

tweets_df=pd.read_csv("train_twitter_data.csv")

tweets_df

tweets_df.describe()

tweets_df['tweet']

tweets_df

sns.heatmap(tweets_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")
 #visualizing tweets with value = NULL

tweets_df.hist(bins = 30, figsize = (13,5), color = 'r')

sns.countplot(tweets_df['label'], label='count')

# Let's get the length of the messages
tweets_df['length']=tweets_df['tweet'].apply(len)

tweets_df

tweets_df['length'].plot(bins=100, kind='hist')

tweets_df.describe()

# to see the shortest message min length
tweets_df[tweets_df['length'] == tweets_df['length'].min()]

tweets_df[tweets_df['length']==84]['tweet'].iloc[0]

#divinding tweets into two dataframes
positive = tweets_df[tweets_df['label']==0]

positive

negative = tweets_df[tweets_df['label']==1]

negative

"""# PLOTTING THE WORDCLOUD"""

sentences=tweets_df['tweet'].tolist()

sentences

len(sentences)
sentences_as_one_string=" ".join(sentences)
sentences_as_one_string

!pip install WordCloud
from wordcloud import WordCloud

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(sentences_as_one_string))
#visual representation of all words we had

negative_list=negative['tweet'].tolist()
negative_sentences_as_one_string=" ".join(negative_list)

plt.figure(figsize=(20,20))
plt.imshow(WordCloud().generate(negative_sentences_as_one_string))
#focussing on the negative sentiment

"""#  **PERFORMING DATA CLEANING**                  

*   REMOVING PUNCTUATION FROM TEXT
*   REMOVING STOPWORDS 


"""

import string
string.punctuation
#remove punc

Test = 'Good morning :)... I am having fun learning Machine learning and AI!!'

Test_punc_removed= [ char for char in Test if char not in string.punctuation ]

Test_punc_removed
# Join the characters again to form the string.

Test_punc_removed=[]
for char in Test:
 if char not in string.punctuation:
     Test_punc_removed.append(char)
        
Test_punc_removed_join=''.join(Test_punc_removed)
Test_punc_removed_join

import nltk # Natural Language tool kit 

nltk.download('stopwords')

# You have to download stopwords Package to execute this command
from nltk.corpus import stopwords
stopwords.words('english')

Test_punc_removed_join_clean=[ word for word in Test_punc_removed_join.split() if word.lower()  not in stopwords.words('english')]

Test_punc_removed_join_clean

Test_punc_removed_join_clean # Only important (no so common) words are left

"""
For the following text, I have followed the following pipeline to remove punctuations followed by removing stopwords"""

mini_challenge = 'Here is a mini challenge, to remove stopwords and punctuations!'

challenge=[ char for char in mini_challenge if char not in string.punctuation]
challenge=''.join(challenge)
challenge=[word for word in challenge.split() if word.lower() not in stopwords.words('english')]

challenge

"""#  PERFORMING COUNT VECTORIZATION (TOKENIZATION)"""

from sklearn.feature_extraction.text import CountVectorizer
sample_data = ['This is the first paper.','This document is the second paper.','And this is the third one.','Is this the first paper?']
vectorizer= CountVectorizer()

X=vectorizer.fit_transform(sample_data)

print(vectorizer.get_feature_names())

X.toarray()

"""# INCORPORATING INTO THE MODEL: CREATE A PIPELINE TO REMOVE PUNCTUATIONS, STOPWORDS AND PERFORM COUNT VECTORIZATION"""

# Let's define a pipeline to clean up all the messages 
# The pipeline performs the following: (1) remove punctuation, (2) remove stopwords

def message_cleaning(message):
    Test_punc_removed = [char for char in message if char not in string.punctuation]
    Test_punc_removed_join = ''.join(Test_punc_removed)
    Test_punc_removed_join_clean = [word for word in Test_punc_removed_join.split() if word.lower() not in stopwords.words('english')]
    return Test_punc_removed_join_clean

# Let's test the newly added function
tweets_df_clean = tweets_df['tweet'].apply(message_cleaning)

print(tweets_df_clean[20]) # show the cleaned up version

print(tweets_df['tweet'][5]) # show the original version

from sklearn.feature_extraction.text import CountVectorizer
# Define the cleaning pipeline we defined earlier
vectorizer = CountVectorizer(analyzer = message_cleaning)
tweets_vectorizer= CountVectorizer(analyzer=message_cleaning, dtype='uint8').fit_transform(tweets_df['tweet']).toarray()
tweets_vectorizer.shape

print(tweets_vectorizer)

tweets = pd.DataFrame(tweets_vectorizer)
tweets

X = tweets

X

y = tweets_df['label']
y

"""#  INTUITION BEHIND THE USE OF NAIVE BAYES"""

#  TRAINING A NAIVE BAYES CLASSIFIER MODEL

X.shape

y.shape

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

from sklearn.naive_bayes import MultinomialNB

NB_classifier = MultinomialNB()
NB_classifier.fit(X_train, y_train)

"""# ASSESS TRAINED MODEL PERFORMANCE  """

from sklearn.metrics import classification_report, confusion_matrix

# Predicting the Test set results
y_predict_test = NB_classifier.predict(X_test)
cm = confusion_matrix(y_test, y_predict_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_test))